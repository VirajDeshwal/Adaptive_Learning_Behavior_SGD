{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import math\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "num_classes = 10\n",
    "epochs = 30\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data() \n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "train_generator = train_datagen.flow(\n",
    "        x_train,\n",
    "        y_train,        \n",
    "        batch_size=batch_size)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = test_datagen.flow(\n",
    "        x_test,\n",
    "        y_test,        \n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    net_input = Input(shape=x_train.shape[1:])\n",
    "    \n",
    "    net = Conv2D(32, (3, 3), padding='same', activation='relu')(net_input)\n",
    "    net = Dropout(0.2)(net)\n",
    "    \n",
    "    net = Conv2D(32,(3,3),padding='same', activation='relu')(net)\n",
    "    net = MaxPooling2D(pool_size=(2,2))(net)\n",
    " \n",
    "    net = Conv2D(64,(3,3),padding='same',activation='relu')(net)\n",
    "    net = Dropout(0.2)(net)\n",
    " \n",
    "    net = Conv2D(64,(3,3),padding='same',activation='relu')(net)\n",
    "    net = MaxPooling2D(pool_size=(2,2))(net)\n",
    " \n",
    "    net = Conv2D(128,(3,3),padding='same',activation='relu')(net)\n",
    "    net = Dropout(0.2)(net)\n",
    " \n",
    "    net = Conv2D(128,(3,3),padding='same',activation='relu')(net)\n",
    "    net = MaxPooling2D(pool_size=(2,2))(net)\n",
    "    \n",
    "    net = Flatten(name='flatten')(net) \n",
    "    net = Dense(1024, activation='relu')(net)\n",
    "    net = Dense(512, activation='relu')(net)\n",
    "    net = Dense(256, activation='relu')(net)\n",
    "    softmax_output = Dense(num_classes, activation='softmax')(net)\n",
    "\n",
    "    model = Model(net_input, softmax_output)\n",
    "    \n",
    "    model.compile(optimizer=SGD(lr=0.0, momentum=0.9, nesterov=True, decay=0.0), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.lr = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.lr.append(step_decay(len(self.losses)))\n",
    "        print(' lr:', step_decay(len(self.losses)))\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.9\n",
    "    epochs_drop = 5.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# learning schedule callback\n",
    "loss_history = LossHistory()\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [loss_history, lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 3,043,882\n",
      "Trainable params: 3,043,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3090 - acc: 0.1080 lr: 0.1\n",
      "100/100 [==============================] - 54s 538ms/step - loss: 2.3091 - acc: 0.1075 - val_loss: 2.3087 - val_acc: 0.1034\n",
      "Epoch 2/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3099 - acc: 0.1067 lr: 0.1\n",
      "100/100 [==============================] - 52s 522ms/step - loss: 2.3098 - acc: 0.1072 - val_loss: 2.3125 - val_acc: 0.1056\n",
      "Epoch 3/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3100 - acc: 0.1070 lr: 0.1\n",
      "100/100 [==============================] - 52s 522ms/step - loss: 2.3102 - acc: 0.1066 - val_loss: 2.3142 - val_acc: 0.1025\n",
      "Epoch 4/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3109 - acc: 0.0963 lr: 0.1\n",
      "100/100 [==============================] - 52s 522ms/step - loss: 2.3108 - acc: 0.0966 - val_loss: 2.3058 - val_acc: 0.0989\n",
      "Epoch 5/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3117 - acc: 0.0928 lr: 0.09000000000000001\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 2.3112 - acc: 0.0928 - val_loss: 2.3107 - val_acc: 0.0966\n",
      "Epoch 6/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3090 - acc: 0.0969 lr: 0.09000000000000001\n",
      "100/100 [==============================] - 54s 537ms/step - loss: 2.3090 - acc: 0.0969 - val_loss: 2.3117 - val_acc: 0.1028\n",
      "Epoch 7/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3109 - acc: 0.0925 lr: 0.09000000000000001\n",
      "100/100 [==============================] - 53s 526ms/step - loss: 2.3108 - acc: 0.0922 - val_loss: 2.3088 - val_acc: 0.1134\n",
      "Epoch 8/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3120 - acc: 0.0900 lr: 0.09000000000000001\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 2.3119 - acc: 0.0906 - val_loss: 2.3087 - val_acc: 0.1044\n",
      "Epoch 9/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3102 - acc: 0.0969 lr: 0.09000000000000001\n",
      "100/100 [==============================] - 56s 559ms/step - loss: 2.3101 - acc: 0.0978 - val_loss: 2.3081 - val_acc: 0.0991\n",
      "Epoch 10/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3074 - acc: 0.1111 lr: 0.08100000000000002\n",
      "100/100 [==============================] - 54s 535ms/step - loss: 2.3074 - acc: 0.1109 - val_loss: 2.3068 - val_acc: 0.0983\n",
      "Epoch 11/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3095 - acc: 0.0912 lr: 0.08100000000000002\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 2.3096 - acc: 0.0913 - val_loss: 2.3080 - val_acc: 0.0984\n",
      "Epoch 12/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3080 - acc: 0.1001 lr: 0.08100000000000002\n",
      "100/100 [==============================] - 53s 528ms/step - loss: 2.3082 - acc: 0.0994 - val_loss: 2.3067 - val_acc: 0.0984\n",
      "Epoch 13/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3070 - acc: 0.0966 lr: 0.08100000000000002\n",
      "100/100 [==============================] - 56s 564ms/step - loss: 2.3068 - acc: 0.0959 - val_loss: 2.3088 - val_acc: 0.0992\n",
      "Epoch 14/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3082 - acc: 0.0975 lr: 0.08100000000000002\n",
      "100/100 [==============================] - 51s 508ms/step - loss: 2.3084 - acc: 0.0966 - val_loss: 2.3052 - val_acc: 0.1106\n",
      "Epoch 15/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3090 - acc: 0.0991 lr: 0.0729\n",
      "100/100 [==============================] - 52s 519ms/step - loss: 2.3089 - acc: 0.0997 - val_loss: 2.3052 - val_acc: 0.1053\n",
      "Epoch 16/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3076 - acc: 0.0919 lr: 0.0729\n",
      "100/100 [==============================] - 52s 523ms/step - loss: 2.3076 - acc: 0.0925 - val_loss: 2.3067 - val_acc: 0.1036\n",
      "Epoch 17/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3064 - acc: 0.1039 lr: 0.0729\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 2.3068 - acc: 0.1037 - val_loss: 2.3101 - val_acc: 0.0944\n",
      "Epoch 18/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3073 - acc: 0.1092 lr: 0.0729\n",
      "100/100 [==============================] - 66s 662ms/step - loss: 2.3076 - acc: 0.1097 - val_loss: 2.3075 - val_acc: 0.0922\n",
      "Epoch 19/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3059 - acc: 0.1032 lr: 0.0729\n",
      "100/100 [==============================] - 58s 583ms/step - loss: 2.3058 - acc: 0.1031 - val_loss: 2.3071 - val_acc: 0.0942\n",
      "Epoch 20/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3083 - acc: 0.0903 lr: 0.06561\n",
      "100/100 [==============================] - 55s 550ms/step - loss: 2.3084 - acc: 0.0900 - val_loss: 2.3065 - val_acc: 0.1009\n",
      "Epoch 21/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3080 - acc: 0.0960 lr: 0.06561\n",
      "100/100 [==============================] - 54s 536ms/step - loss: 2.3079 - acc: 0.0956 - val_loss: 2.3040 - val_acc: 0.1019\n",
      "Epoch 22/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3069 - acc: 0.1016 lr: 0.06561\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 2.3067 - acc: 0.1013 - val_loss: 2.3055 - val_acc: 0.1027\n",
      "Epoch 23/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3079 - acc: 0.0938 lr: 0.06561\n",
      "100/100 [==============================] - 54s 540ms/step - loss: 2.3078 - acc: 0.0941 - val_loss: 2.3072 - val_acc: 0.1016\n",
      "Epoch 24/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3065 - acc: 0.1013 lr: 0.06561\n",
      "100/100 [==============================] - 52s 522ms/step - loss: 2.3067 - acc: 0.1009 - val_loss: 2.3084 - val_acc: 0.1013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3069 - acc: 0.1013 lr: 0.05904900000000001\n",
      "100/100 [==============================] - 52s 520ms/step - loss: 2.3068 - acc: 0.1009 - val_loss: 2.3075 - val_acc: 0.1019\n",
      "Epoch 26/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3069 - acc: 0.1057 lr: 0.05904900000000001\n",
      "100/100 [==============================] - 56s 559ms/step - loss: 2.3071 - acc: 0.1047 - val_loss: 2.3065 - val_acc: 0.1005\n",
      "Epoch 27/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3079 - acc: 0.0944 lr: 0.05904900000000001\n",
      "100/100 [==============================] - 54s 538ms/step - loss: 2.3079 - acc: 0.0947 - val_loss: 2.3054 - val_acc: 0.1006\n",
      "Epoch 28/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3061 - acc: 0.1007 lr: 0.05904900000000001\n",
      "100/100 [==============================] - 55s 549ms/step - loss: 2.3062 - acc: 0.1006 - val_loss: 2.3101 - val_acc: 0.0934\n",
      "Epoch 29/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3075 - acc: 0.0956 lr: 0.05904900000000001\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 2.3075 - acc: 0.0959 - val_loss: 2.3035 - val_acc: 0.1055\n",
      "Epoch 30/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 2.3062 - acc: 0.1051 lr: 0.05314410000000001\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 2.3062 - acc: 0.1044 - val_loss: 2.3061 - val_acc: 0.0950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182112d828>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= 100,\n",
    "        epochs=epochs,callbacks=callbacks_list,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nISIiMSlZiIhITEoWIiIS0/8HgAgKcUYqijIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1822d7ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning rate\n",
    "fig = plt.figure()\n",
    "plt.plot(range(1,epochs+1),loss_history.lr,label='learning rate')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.xlim([1,epochs+1])\n",
    "plt.ylabel(\"learning rate\")\n",
    "plt.legend(loc=0)\n",
    "plt.grid(True)\n",
    "plt.title(\"Learning rate\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
